{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOI67VY3HQH-"
      },
      "outputs": [],
      "source": [
        "# TWITTER CLUSTERING TASKS WITH KEYWORDS \"SHIN TAE YONG\"\n",
        "# START : 2022-10-01, AND END : 2022-10-15"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS FIRST\n",
        "!pip3 install snscrape"
      ],
      "metadata": {
        "id": "sCvfj6JtHpnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi **\"snscrape\"** adalah :\n",
        "Untuk layanan jejaring sosial (SNS). Seperti mengikis hal-hal seperti profil pengguna, tagar, atau pencarian dan mengembalikan item yang ditemukan.\n",
        "Contohnya posting yang relevan."
      ],
      "metadata": {
        "id": "nmyK4HulI6vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "cgmlE9wZIOc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi **\"langdetect\"** adalah untuk mengimplementasi ulang pustaka deteksi bahasa Google ke Python"
      ],
      "metadata": {
        "id": "XiUKmnztJFeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS PROCESS IS USEFUL FOR IMPORTING THE SNSCRAPE MODULE TO HANDLE THE DATA WE GET FROM TWITTER\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import json\n",
        "from langdetect import detect"
      ],
      "metadata": {
        "id": "9s9kafTUJhZY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CHOOSE THE RIGHT KEYWORDS\n",
        "keywords=['shin tae yong']\n",
        "start=\"2022–10–01\"\n",
        "end =\"2022–10–15\"\n",
        "max_num=100\n",
        "fname='tweet.json' \n",
        "languages=['id','en']"
      ],
      "metadata": {
        "id": "5anS_F4tJwV_"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE VARIABLE \"datatw\" FOR, DISPLAY RESULTS OF DATA TAKEN ON TWITTER\n",
        "import pandas as pd\n",
        "datatw=[]"
      ],
      "metadata": {
        "id": "sdEUwOnBJ2RK"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi **Library Panda** : untuk membuat tabel, mengubah dimensi data, mengecek data, dan lain sebagainya. Karena pada Library Panda menyediakan struktur data dan analisis data yang mudah digunakan.\n"
      ],
      "metadata": {
        "id": "ZxNH_vb8bqkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for keyword in keywords:\n",
        "   \n",
        "    for i, tweet in enumerate (sntwitter.TwitterSearchScraper(f'{keyword} ').get_items()):\n",
        "        \n",
        "        try:\n",
        "            lan=detect(tweet.content)\n",
        "        except:\n",
        "            lan='error'\n",
        "        if i == max_num:\n",
        "            break\n",
        "        if lan in languages:\n",
        "            data = {'id': tweet.id, 'username':tweet.username, 'date': tweet.date, 'text': tweet.content,'url':tweet.url}\n",
        "           # print(data)\n",
        "            datatw.append(tweet.content)\n",
        "            with open(fname, 'a+', encoding='utf-8') as f:\n",
        "                line = json.dumps(data, ensure_ascii=False,default=str)\n",
        "                #print(line)\n",
        "                f.write(line)\n",
        "                f.write('\\n')"
      ],
      "metadata": {
        "id": "-XnouDNsJ4He"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datatw"
      ],
      "metadata": {
        "id": "bBo7EWZRJ8Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "Sba-5627J_X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi **Install Sastrawi** : untuk mengurangi kata-kata yang terinfleksi dalam Bahasa Indonesia (Bahasa Indonesia) ke bentuk dasarnya (batang)."
      ],
      "metadata": {
        "id": "ZrwYEkpWeUl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# \"StopWordRemoverFactory\" IS ONE OF THE LIBRARY OF LITERATURE WE HAVE INSTALL\n",
        "import re\n",
        "import string\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()# stemming process\n",
        "# import StopWordRemoverFactory class\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "factory = StopWordRemoverFactory()\n",
        "stopword = factory.create_stop_word_remover()\n",
        "documents_clean=[]\n",
        "\n",
        "for d in datatw:\n",
        "    outputstem= stemmer.stem(d)\n",
        "    d= stopword.remove(outputstem)\n",
        "    # Remove Unicode\n",
        "    document_test = re.sub(r'[^\\x00-\\x7F]+', ' ', d)\n",
        "    # Remove Mentions\n",
        "    document_test = re.sub(r'@\\w+', '', document_test)\n",
        "    # Lowercase the document\n",
        "    document_test = document_test.lower()\n",
        "    # Remove punctuations\n",
        "    document_test = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', document_test)\n",
        "    # Lowercase the numbers\n",
        "    document_test = re.sub(r'[0-9]', '', document_test)\n",
        "    # Remove the doubled space\n",
        "    outputstop = re.sub(r'\\s{2,}', ' ', document_test)\n",
        "    documents_clean.append(outputstop)"
      ],
      "metadata": {
        "id": "7NYxwJA_KDaf"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents_clean[0:5]"
      ],
      "metadata": {
        "id": "Ycy4Je3GKKHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi **\"documents_clean\"** adalah : untuk menghapus semua karakter tidak dapat dicetak dari teks"
      ],
      "metadata": {
        "id": "89cvajgMjBjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
        "tfidf_wm = tfidfvectorizer.fit_transform(documents_clean)\n",
        "tfidf_tokens = tfidfvectorizer.get_feature_names()"
      ],
      "metadata": {
        "id": "i2DReIMLKNfd"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi **\"TfidfVectorizer\"** adalah : sebagai normalisasi nilai bobot yang diperoleh, supaya nilai bobot berada pada range yang sama."
      ],
      "metadata": {
        "id": "gCLecbpgjap2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "cv = CountVectorizer()\n",
        "words = cv.fit_transform(documents_clean)\n",
        "sum_words = words.sum(axis=0)\n",
        "\n",
        "\n",
        "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
        "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
        "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
        "\n",
        "color = plt.cm.twilight(np.linspace(0, 1, 20))\n",
        "frequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = color)\n",
        "plt.title(\"Most Frequently Occuring Words - Top 20\")"
      ],
      "metadata": {
        "id": "s8Bad3mgKSCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi **\"CountVectorizer\"** adalah : untuk menghitung frekuensi kata dalam dokumen. Count Vectorizer dapat mengubah fitur teks menjadi sebuah representasi vector."
      ],
      "metadata": {
        "id": "OjjaFgj9j_tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT K-MEANS FOR CLUSTERING PROCESS\n",
        "from sklearn.cluster import KMeans\n",
        "true_k = 3\n",
        "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
        "model.fit(words)"
      ],
      "metadata": {
        "id": "vSxMkpD9KZZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO ORDER CENTROID, TO DISTRIBUTE RESULTS FOR EVERY CLUSTER\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = cv.get_feature_names()\n",
        "\n",
        "for i in range(true_k):\n",
        "    print(\"Cluster %d:\" % i),\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(' %s' % terms[ind]),\n",
        "    print\n",
        "\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "BTjuK9zSKdHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO IT TO SEE THE DENDOGRAM DIAGRAM\n",
        "import scipy.cluster.hierarchy as sch\n",
        "X = cv.fit_transform(documents_clean).todense()\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'ward',metric='euclidean'),orientation=\"top\")\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Jarak Ward')\n",
        "plt.ylabel('Nomor Dokumen')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k6cbntHjKg8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fungsi Method **\"ward\"** adalah : suatu metode pembentukan cluster yang di dasari oleh hilangnya informasi akibat penggabungan obyek menjadi cluster."
      ],
      "metadata": {
        "id": "03Rfzhm_fws1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO IT TO SEE THE DENDOGRAM DIAGRAM\n",
        "import scipy.cluster.hierarchy as sch\n",
        "X = cv.fit_transform(documents_clean).todense()\n",
        "dendrogram = sch.dendrogram(sch.linkage(X, method = 'average',metric='euclidean'),orientation=\"right\")\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Jarak Rerata')\n",
        "plt.ylabel('Nomor Dokumen')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gZIhQzekKomN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}